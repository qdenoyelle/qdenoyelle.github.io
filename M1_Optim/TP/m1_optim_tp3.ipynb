{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Optimisation M1 - TP3</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Débruitage</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def produitScalaireMatrice(x, y):\n",
    "    return np.sum(x*y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va chercher dans cette partie à mettre au point un algorithme de débruitage d'une image via la résolution d'un problème d'optimisation par la méthode du gradient conjugué. L'image source (non bruitée) est donnée dans le code suivant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8, 6))\n",
    "img_original = plt.imread('bouquetin.png').astype(float) # chargement de l'image\n",
    "img_original = sum([img_original[:, :, i] for i in range(2)]) # on transforme l'image en noir et blanc\n",
    "plt.imshow(img_original, cmap = 'gray') # affichage de l'image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va maintenant dégrader l'image par un bruit additif gaussien (en pratique on aurait uniquement accès à l'image bruitée, ici on cré \"artificiellement\" le bruit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n = img_original.shape # m représente le nombre de ligne de l'image, n le nombre de colonne\n",
    "bruit = .15 * np.random.randn(m, n) # bruit gaussien qui va être appliqué sur l'image\n",
    "img_bruite = img_original + bruit # on bruite additivement l'image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Afficher l'image bruitée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans la suite les images seront vues comme des tableaux de valeur (chaque pixel est un nombre représentant le niveau de gris), c'est à dire des matrices à $m$ lignes et $n$ colonnes. Dans le cours nous avons vu la résolution de problème d'optimisation sur $\\mathbb{R}^p$, on notera que l'on peut considérer sans ambiguité des problèmes d'optimisation sur $\\mathcal{M}_{m,n}(\\mathbb{R})$ puisque cette espace vectoriel est isomorphe à $\\mathbb{R}^{mn}$.\n",
    "\n",
    "On va maintenant chercher à débruiter l'image ci-dessus (i.e. diminuer l'impact du bruit). La méthode ici adopter va consister à résoudre le problème d'optimisation suivant\n",
    "$$\n",
    "\\min_{x\\in\\mathcal{M}_{m,n}(\\mathbb{R})} F(x),\n",
    "$$\n",
    "où\n",
    "$$\n",
    "F(x) = \\frac12 \\sum_{i=1}^m\\sum_{j=1}^n (x_{i,j} - y_{i,j})^2 + \\frac{\\lambda}2 \\sum_{i=1}^{m-1}\\sum_{j=1}^n (x_{i+1,j} - x_{i,j})^2 + \\frac{\\lambda}2 \\sum_{i=1}^{m}\\sum_{j=1}^{n-1} (x_{i,j+1} - x_{i,j})^2.\n",
    "$$\n",
    "$y\\in\\mathcal{M}_{m,n}(\\mathbb{R})$ est l'image bruitée (la donnée du problème) et $\\lambda>0$ une constante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Quel est la dimension de ce problème d'optimisation ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Interpréter $F$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Démontrer que $F$ peut s'écrire sous la forme\n",
    "$$\n",
    "\\forall x\\in\\mathcal{M}_{m,n}(\\mathbb{R}), \\quad F(x) = \\frac12 \\langle \\mathcal{A}(x), x \\rangle - \\langle b, x\\rangle + c,\n",
    "$$\n",
    "où\n",
    "<ul>\n",
    "<li>$\\langle \\cdot, \\cdot \\rangle$ est le produit scalaire canonique sur l'espace $\\mathcal{M}_{m,n}(\\mathbb{R})$, c'est-à-dire $\\langle x, x' \\rangle = \\mbox{Tr}(x^Tx') = \\sum_{i,j} x_{i,j}x'_{i,j}$,</li>\n",
    "<li>$\\mathcal{A}:\\mathcal{M}_{m,n}(\\mathbb{R})\\to\\mathcal{M}_{m,n}(\\mathbb{R})$ un opérateur linéaire tel que $\\mathcal{A}(x) = x + \\lambda L^TL x + \\lambda xCC^T$, avec $L\\in\\mathcal{M}_{m,m}(\\mathbb{R})$ la matrice avec des $-1$ sur la diagonale, $1$ sur la sur-diagonale et $L_{m,m}=0$, et $C\\in\\mathcal{M}_{n,n}(\\mathbb{R})$ la matrice avec des $-1$ sur la diagonale, $1$ sur la sous-diagonale et $C_{n,n}=0$,</li>\n",
    "<li>$b=y\\in\\mathcal{M}_{m,n}(\\mathbb{R})$ et $c = \\frac12 \\|y\\|_2^2$.</li>\n",
    "</lu>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Démontrer que $\\mathcal{A}$ est un endomorphisme symétrique, c'est-à-dire pour tous $x,x'\\in\\mathcal{M}_{m,n}(\\mathbb{R})$, $\\langle \\mathcal{A}(x), x' \\rangle = \\langle x, \\mathcal{A}(x') \\rangle$ puis que $\\mathcal{A}$ est définie positive, c'est-à-dire pour tous $x\\neq0$, $\\langle \\mathcal{A}(x), x \\rangle >0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Que peut-on en conclure sur $F$ ? Et justifier que $F$ admet un unique minimum global."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) Ecrire une fonction $\\mbox{opA}$ qui prend comme argument $x$ et qui renvoie $\\mathcal{A}(x)$. On pourra commencer par créer les deux matrices $L$ et $C$ à l'aide de la commande np.eye (penser à regarder l'effet par exemple de np.eye(4) et np.eye(4, k = 1))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va maintenant utiliser l'algorithme du gradient conjuguée pour trouver le minimum global de "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) Ecrire une fonction $\\mbox{gradientConjugue}$, qui prend en argument $\\mbox{A}$ un opérateur linéaire, $\\mbox{b}$, $\\mbox{x0}$ un point de départ, $\\mbox{tol}$ une tolérance et $\\mbox{Niter}$ un nombre maximal d'itérations. Et qui renvoie l'approximée du minimum global et la liste des normes des gradients.\n",
    "\n",
    "On pensera à utiliser la fonction $\\mbox{produitScalaireMatrice}$, définit au tout début du TP, pour calculer les produits scalaires entre deux matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9) Utiliser l'algorithme pour le résoudre le problème d'optimisation. On prendra pour l'instant $\\lambda = 2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10) Illustrer la convergence de l'algorithme. Commenter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11) Discuter de l'influence de $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12) Résoudre le même problème avec la méthode du gradient à pas optimal. Comparer les deux convergences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Bonus</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13) Donner le nombre de fois que l'on applique l'opérateur $\\mbox{A}$ dans la boucle principale des deux algorithmes ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14) Il est possible d'écrire une version de ces algorithmes où $\\mbox{A}$ n'est appliquée qu'une seule fois par itération (et donc d'améliorer significativement la vitesse de l'algorithme (meilleure complexité)). Pour cela calculer $\\nabla F(x_{k+1}) - \\nabla F(x_k)$ pour les deux algithmes et montrer que le calcul du nouveau gradient $\\nabla F(x_{k+1})$ peut ainsi utiliser le précédent et une application de l'opérateur $\\mbox{A}$ déjà calculé dans la boucle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15) Mettre à jour les deux algorithmes en utilisant cette optimisation. Tester."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16**) Une dernière optimisation réalisable est d'écrire l'opérateur $\\mbox{A}$ en faisant uniquement des opérateurs directement sur la matrice $x$ (opérations sur les lignes et colonnes). Donc sans avoir besoin de créer les matrices $L$ et $C$ et de les appliquer (ces matrices contiennent essentiellement que des $0$ donc les opération $L^TLx$ et $xDD^T$ sont inutilement couteuses). Ecrire une nouvelle version de $\\mbox{opA}$ plus optimisée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
